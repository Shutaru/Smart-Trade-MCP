# ?? BACKTEST ENGINE - VALIDATION METHODS

**Component:** `src/core/backtest_engine.py`  
**Purpose:** Advanced validation methods for trading strategies  
**Status:** ? Production Ready

---

## ?? OVERVIEW

The `BacktestEngine` provides three levels of validation:

1. **Basic Backtest** - Standard in-sample testing
2. **Walk-Forward Analysis** - Out-of-sample validation (rolling windows)
3. **K-Fold Validation** - Cross-validation across different periods
4. **Monte Carlo Simulation** - Risk analysis from trade sequences

---

## ?? WALK-FORWARD ANALYSIS

### Method Signature

```python
def walk_forward_analysis(
    self,
    strategy: BaseStrategy,
    df: pd.DataFrame,
    train_days: int = 180,
    test_days: int = 60,
    step_days: int = 30,
    optimize_func: Optional[Callable] = None,
    parallel: bool = True,
    n_jobs: int = -1,
) -> Dict[str, Any]:
```

### Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `strategy` | `BaseStrategy` | - | Trading strategy to validate |
| `df` | `pd.DataFrame` | - | Historical OHLCV data with indicators |
| `train_days` | `int` | 180 | Training window size (days) |
| `test_days` | `int` | 60 | Testing window size (days) |
| `step_days` | `int` | 30 | Rolling step size (days) |
| `optimize_func` | `Callable` | None | Optional optimization function |
| `parallel` | `bool` | True | Execute windows in parallel |
| `n_jobs` | `int` | -1 | Number of CPU cores (-1 = all) |

### Returns

```python
{
    "windows": List[Dict],           # Individual window results
    "n_windows": int,                # Number of windows analyzed
    "stability_ratio": float,        # out_sample / in_sample return
    "consistency": float,            # % profitable out-sample windows
    "avg_in_sample_return": float,   # Average training return
    "avg_out_sample_return": float,  # Average testing return
    "std_in_sample_return": float,   # Std dev training
    "std_out_sample_return": float,  # Std dev testing
    "best_window": Dict,             # Best performing window
    "worst_window": Dict,            # Worst performing window
    "recommendation": str,           # PASS/MARGINAL/FAIL
}
```

### Recommendation Logic

```python
if stability_ratio >= 0.7 and consistency >= 70 and avg_out_sample > 0:
    recommendation = "PASS - Strategy validated, ready for optimization"
elif stability_ratio >= 0.5 and consistency >= 50:
    recommendation = "MARGINAL - Consider parameter tuning"
else:
    recommendation = "FAIL - Likely overfitting, do not use in production"
```

### Example Usage

```python
from src.core.backtest_engine import BacktestEngine
from src.strategies import registry

# Get strategy
strategy = registry.get("cci_extreme_snapback")

# Create engine
engine = BacktestEngine(initial_capital=10000.0)

# Run WFA
results = engine.walk_forward_analysis(
    strategy=strategy,
    df=market_data,
    train_days=180,
    test_days=60,
    step_days=30,
    parallel=True,
)

# Interpret results
print(f"Stability Ratio: {results['stability_ratio']:.2f}")
print(f"Consistency: {results['consistency']:.1f}%")
print(f"Recommendation: {results['recommendation']}")

if results['recommendation'].startswith("PASS"):
    print("? Strategy validated!")
else:
    print("? Strategy failed validation")
```

---

## ?? INTERNAL METHODS

### `_create_wfa_windows()`

Creates rolling train/test windows from historical data.

**Logic:**
1. Start at beginning of dataset
2. Create window: [train_start, train_end, test_start, test_end]
3. Advance by `step_days`
4. Repeat until insufficient data

**Overlap:**
- If `step_days < test_days`: windows overlap (recommended)
- If `step_days == test_days`: no overlap
- If `step_days > test_days`: gap between windows (not recommended)

### `_execute_wfa_serial()`

Executes WFA windows sequentially (single-threaded).

**Use when:**
- Only 1-2 windows
- Debugging
- Limited memory

### `_execute_wfa_parallel()`

Executes WFA windows in parallel using multiple CPU cores.

**Implementation:**
- Uses `ProcessPoolExecutor` (multi-process)
- Each window runs in separate process
- Automatic load balancing
- Graceful error handling

**Performance:**
- 4-core CPU: ~3-4x faster
- 8-core CPU: ~6-7x faster
- 16-core CPU: ~12-14x faster

### `_execute_single_window()`

Executes a single WFA window (train + test).

**Steps:**
1. Optionally optimize parameters on training data
2. Backtest on training data (in-sample)
3. Backtest on testing data (out-of-sample)
4. Return metrics for both

### `_execute_single_window_static()`

Static wrapper for parallel execution.

**Why static?**
- Instance methods can't be pickled for multiprocessing
- Static methods can be pickled and sent to worker processes

### `_analyze_wfa_results()`

Aggregates results from all windows.

**Calculates:**
- Stability ratio
- Consistency percentage
- Average returns
- Standard deviations
- Best/worst windows
- Recommendation

---

## ?? PERFORMANCE OPTIMIZATION

### Parallel Execution

**Enabled by default:**
```python
results = engine.walk_forward_analysis(
    strategy=strategy,
    df=df,
    parallel=True,    # ? Default
    n_jobs=-1,        # ? Use all CPUs
)
```

**Manual control:**
```python
results = engine.walk_forward_analysis(
    strategy=strategy,
    df=df,
    parallel=True,
    n_jobs=4,         # ? Use exactly 4 cores
)
```

**Disable for debugging:**
```python
results = engine.walk_forward_analysis(
    strategy=strategy,
    df=df,
    parallel=False,   # ? Sequential execution
)
```

### Memory Management

- Each window processes a copy of its data slice
- Original DataFrame is not modified
- Memory usage: ~O(n_windows * window_size)
- For large datasets, reduce `train_days` or increase `step_days`

---

## ?? COMMON PITFALLS

### 1. Insufficient Data

? **Wrong:**
```python
# Only 200 days of data
results = engine.walk_forward_analysis(
    train_days=180,
    test_days=60,
)
# Result: Only 1 window ? unreliable
```

? **Correct:**
```python
# At least 1 year of data
# Gives ~5-6 windows with above params
```

### 2. Look-Ahead Bias

? **Wrong:**
```python
# Strategy uses future data
df['signal'] = df['close'].shift(-1)  # Peek ahead!
```

? **Correct:**
```python
# Only use past data
df['signal'] = df['close'].shift(1)
```

### 3. Ignoring Recommendations

? **Wrong:**
```python
if results['recommendation'] == "FAIL":
    print("Deploying anyway!")  # ?? Recipe for disaster
```

? **Correct:**
```python
if results['recommendation'] == "PASS":
    print("Proceeding to optimization")
else:
    print("Back to the drawing board")
```

---

## ?? INTEGRATION WITH OPTIMIZATION

### Phase 2: Adaptive Parameters

```python
from src.core.optimization_engine import GeneticAlgorithm

def optimize_on_train(strategy, train_df):
    """Optimize parameters on training window."""
    ga = GeneticAlgorithm(population=100, generations=50)
    
    optimized_params = ga.optimize(
        strategy=strategy,
        df=train_df,
        objective="sharpe_ratio",
    )
    
    # Return new strategy with optimized params
    return strategy.with_params(optimized_params)

# Run WFA with per-window optimization
results = engine.walk_forward_analysis(
    strategy=base_strategy,
    df=df,
    optimize_func=optimize_on_train,  # ? Adaptive!
)
```

This enables **adaptive strategies** that re-optimize in each window.

---

## ?? REAL-WORLD VALIDATION

### Example: Validating "CCI Extreme Snapback"

**Initial Backtest:** +8.75% annual return, 57.6% WR

**Question:** Real or overfitted?

**WFA Test:**
```python
results = engine.walk_forward_analysis(
    strategy=cci_strategy,
    df=one_year_data,
    train_days=180,
    test_days=60,
    step_days=30,
)
```

**Possible Outcomes:**

**Scenario A - Validated Strategy:**
```
Stability Ratio: 0.85  ?
Consistency: 80%       ?
Recommendation: PASS

? Strategy is genuinely profitable!
? Proceed to parameter optimization
```

**Scenario B - Overfitted Strategy:**
```
Stability Ratio: 0.35  ?
Consistency: 40%       ?
Recommendation: FAIL

? Strategy is curve-fitted to historical data
? DO NOT deploy to production
? Redesign strategy
```

---

## ?? ACADEMIC FOUNDATION

### Research Papers:

1. **"Pseudo-Mathematics and Financial Charlatanism"**
   - Bailey, Borwein, López de Prado, Zhu (2014)
   - Emphasizes out-of-sample testing

2. **"The Probability of Backtest Overfitting"**
   - Bailey, Borwein, López de Prado, Zhu (2016)
   - Mathematical framework for overfitting

3. **"Walk-Forward Analysis"**
   - Robert Pardo (2008)
   - Original WFA methodology

### Key Insight:

> "A strategy that performs well in backtest but fails walk-forward analysis
> is statistically indistinguishable from random noise curve-fitted to
> historical data." - López de Prado

---

## ?? TROUBLESHOOTING

### Issue: `ValueError: Insufficient data`

**Cause:** Not enough data for requested windows

**Solution:**
```python
# Fetch more data
df = await dm.fetch_ohlcv(limit=1000)

# Or reduce window sizes
results = engine.walk_forward_analysis(
    train_days=120,  # Smaller
    test_days=40,    # Smaller
)
```

### Issue: WFA very slow

**Cause:** Running sequentially or too many windows

**Solution:**
```python
# Enable parallelism
results = engine.walk_forward_analysis(
    parallel=True,
    n_jobs=-1,  # All cores
)

# Or increase step size
results = engine.walk_forward_analysis(
    step_days=60,  # Larger step = fewer windows
)
```

### Issue: Inconsistent results

**Cause:** Data quality, random initialization, or unstable strategy

**Solution:**
```python
# Check data quality
print(df.isnull().sum())

# Set random seed for reproducibility
np.random.seed(42)

# Re-run with different parameters
```

---

## ?? FURTHER READING

- See `docs/VALIDATION.md` for conceptual overview
- See `examples/walk_forward_example.py` for full example
- See tests for edge cases and validation

---

**Last Updated:** 2025-11-20  
**Version:** 1.0.0  
**Status:** ? Production Ready
