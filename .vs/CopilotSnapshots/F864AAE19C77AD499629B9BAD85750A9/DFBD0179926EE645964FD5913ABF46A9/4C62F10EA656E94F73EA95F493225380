# -*- coding: utf-8 -*-
"""
Genetic Algorithm Optimizer

Modern implementation using DEAP with Rich CLI dashboard integration.

Features:
- DEAP genetic algorithm framework
- Multi-objective optimization (Sharpe, Win Rate, Max DD)
- Rich CLI progress dashboard
- Type-safe parameter handling
- GPU acceleration support (optional)
- Ray distributed computing (optional)
- Database storage of results
"""

from typing import Dict, Any, List, Tuple, Optional, Callable
import random
import time
from datetime import datetime
import pandas as pd
from pathlib import Path
import sys
import os

try:
    from deap import base, creator, tools, algorithms
    DEAP_AVAILABLE = True
except ImportError:
    DEAP_AVAILABLE = False
    print("⚠️  DEAP not available. Install with: pip install deap")

from rich.console import Console
from rich.live import Live

from .config import OptimizationConfig
from .parameter_space import ParameterSpace, ParameterType
from .fitness_evaluator import FitnessEvaluator, FitnessMetrics
from ..core.cli_dashboard import OptimizationDashboard
from ..core.logger import logger

# Fix Windows encoding issues
if sys.platform == 'win32':
    # Try to set UTF-8 encoding
    try:
        # For Python 3.7+
        if hasattr(sys.stdout, 'reconfigure'):
            sys.stdout.reconfigure(encoding='utf-8')
            sys.stderr.reconfigure(encoding='utf-8')
    except:
        pass
    
    # Set environment variable
    os.environ['PYTHONIOENCODING'] = 'utf-8'

# Rich console for live display with safe encoding AND legacy mode
console = Console(
    force_terminal=True,
    legacy_windows=True,  # Better PowerShell compatibility
    emoji=False,
    no_color=False,
)

class GeneticOptimizer:
    """
    Genetic Algorithm optimizer for trading strategies.
    
    Uses DEAP for GA implementation and Rich for beautiful CLI progress display.
    """
    
    def __init__(
        self,
        df: pd.DataFrame,
        strategy_class: Any,
        param_space: ParameterSpace,
        config: Optional[OptimizationConfig] = None,
    ):
        """
        Initialize genetic optimizer.
        
        Args:
            df: OHLCV DataFrame with indicators
            strategy_class: Strategy class to optimize
            param_space: Parameter space definition
            config: Optimization configuration (uses defaults if None)
        """
        if not DEAP_AVAILABLE:
            raise ImportError("DEAP is required for genetic optimization. Install with: pip install deap")
        
        self.df = df
        self.strategy_class = strategy_class
        self.param_space = param_space
        self.config = config or OptimizationConfig()
        
        # Fitness evaluator
        self.evaluator = FitnessEvaluator(
            df=df,
            strategy_class=strategy_class,
            initial_capital=self.config.initial_capital,
            commission=self.config.commission,
            slippage=self.config.slippage,
            use_gpu=self.config.use_gpu,
        )
        
        # Rich dashboard
        self.dashboard = OptimizationDashboard(
            population_size=self.config.population_size,
            n_generations=self.config.n_generations,
            n_gpus=self.config.n_workers if self.config.use_gpu else 0,
            strategy_name=strategy_class.name if hasattr(strategy_class, 'name') else strategy_class.__class__.__name__
        )
        
        # History tracking
        self.history = {
            "best_fitness": [],
            "avg_fitness": [],
            "best_params": [],
            "generation_times": [],
        }
        
        # Best results
        self.best_individual: Optional[List] = None
        self.best_fitness: Optional[FitnessMetrics] = None
        self.best_params: Optional[Dict[str, Any]] = None
        
        # Setup DEAP
        self._setup_deap()
        
        logger.info(
            "GeneticOptimizer initialized",
            strategy=strategy_class.name if hasattr(strategy_class, 'name') else strategy_class.__class__.__name__,
            population=self.config.population_size,
            generations=self.config.n_generations,
            parameters=len(param_space)
        )
    
    def _setup_deap(self):
        """Setup DEAP genetic algorithm framework"""
        
        # Create fitness class (multi-objective: maximize Sharpe, maximize WR, minimize DD)
        if not hasattr(creator, "FitnessMulti"):
            creator.create("FitnessMulti", base.Fitness, weights=self.config.fitness_weights)
        
        if not hasattr(creator, "Individual"):
            creator.create("Individual", list, fitness=creator.FitnessMulti)
        
        self.toolbox = base.Toolbox()
        
        # Register parameter generators
        param_names = list(self.param_space.parameters.keys())
        
        for param_name in param_names:
            param_def = self.param_space[param_name]
            
            if param_def.type == ParameterType.INT:
                self.toolbox.register(
                    f"attr_{param_name}",
                    random.randint,
                    int(param_def.low),
                    int(param_def.high)
                )
            
            elif param_def.type == ParameterType.FLOAT:
                self.toolbox.register(
                    f"attr_{param_name}",
                    random.uniform,
                    float(param_def.low),
                    float(param_def.high)
                )
            
            elif param_def.type == ParameterType.CHOICE:
                self.toolbox.register(
                    f"attr_{param_name}",
                    random.choice,
                    param_def.choices
                )
            
            elif param_def.type == ParameterType.BOOL:
                self.toolbox.register(
                    f"attr_{param_name}",
                    random.choice,
                    [True, False]
                )
        
        # Register individual and population
        self.toolbox.register(
            "individual",
            tools.initCycle,
            creator.Individual,
            tuple([getattr(self.toolbox, f"attr_{name}") for name in param_names]),
            n=1
        )
        
        self.toolbox.register(
            "population",
            tools.initRepeat,
            list,
            self.toolbox.individual
        )
        
        # Register genetic operators
        n_params = len(param_names)
        
        if n_params == 1:
            self.toolbox.register("mate", tools.cxUniform, indpb=0.5)
        elif n_params == 2:
            self.toolbox.register("mate", tools.cxOnePoint)
        else:
            self.toolbox.register("mate", tools.cxTwoPoint)
        
        self.toolbox.register("mutate", self._custom_mutate)
        self.toolbox.register("select", tools.selNSGA2)  # Multi-objective selection
        self.toolbox.register("evaluate", self._evaluate_individual)
    
    def _custom_mutate(self, individual: List) -> Tuple[List]:
        """Custom mutation that respects parameter types and bounds"""
        param_names = list(self.param_space.parameters.keys())
        
        for i, param_name in enumerate(param_names):
            if random.random() < self.config.mutation_prob:
                param_def = self.param_space[param_name]
                
                if param_def.type == ParameterType.INT:
                    individual[i] = random.randint(int(param_def.low), int(param_def.high))
                
                elif param_def.type == ParameterType.FLOAT:
                    # Gaussian mutation with bounds
                    mu = individual[i]
                    sigma = (param_def.high - param_def.low) * 0.1
                    individual[i] = max(param_def.low, min(param_def.high, random.gauss(mu, sigma)))
                
                elif param_def.type == ParameterType.CHOICE:
                    individual[i] = random.choice(param_def.choices)
                
                elif param_def.type == ParameterType.BOOL:
                    individual[i] = not individual[i]
        
        return (individual,)
    
    def _individual_to_params(self, individual: List) -> Dict[str, Any]:
        """Convert DEAP individual to parameter dictionary"""
        param_names = list(self.param_space.parameters.keys())
        return {name: value for name, value in zip(param_names, individual)}
    
    def _evaluate_individual(self, individual: List) -> Tuple[float, float, float]:
        """Evaluate fitness of an individual"""
        params = self._individual_to_params(individual)
        metrics = self.evaluator.evaluate(params)
        return metrics.to_tuple()
    
    def optimize(self) -> Dict[str, Any]:
        """
        Run genetic algorithm optimization.
        
        Returns:
            Dictionary with optimization results
        """
        start_time = time.time()
        
        # Initialize population
        population = self.toolbox.population(n=self.config.population_size)
        
        # Start dashboard
        self.dashboard.start()
        
        # Run optimization with Live dashboard (legacy mode for PowerShell compatibility)
        with Live(self.dashboard.render(), console=console, refresh_per_second=2) as live:
            # Evolution loop
            for gen in range(1, self.config.n_generations + 1):
                gen_start = time.time()
                
                # Evaluate population
                fitnesses = list(map(self.toolbox.evaluate, population))
                for ind, fit in zip(population, fitnesses):
                    ind.fitness.values = fit
                
                # Update dashboard
                self.dashboard.update_generation(gen, len(population))
                
                # Track best individual
                best_ind = tools.selBest(population, 1)[0]
                best_fit_tuple = best_ind.fitness.values
                best_params = self._individual_to_params(best_ind)
                
                # Create FitnessMetrics from tuple
                best_metrics_dict = {
                    "sharpe_ratio": best_fit_tuple[0],
                    "win_rate": best_fit_tuple[1],
                    "max_drawdown": best_fit_tuple[2],
                }
                
                # Calculate average fitness
                avg_sharpe = sum(ind.fitness.values[0] for ind in population) / len(population)
                avg_wr = sum(ind.fitness.values[1] for ind in population) / len(population)
                avg_dd = sum(ind.fitness.values[2] for ind in population) / len(population)
                
                avg_metrics_dict = {
                    "sharpe_ratio": avg_sharpe,
                    "win_rate": avg_wr,
                    "max_drawdown": avg_dd,
                }
                
                # Complete generation
                self.dashboard.complete_generation(best_metrics_dict, avg_metrics_dict)
                
                # Store history
                self.history["best_fitness"].append(best_metrics_dict)
                self.history["avg_fitness"].append(avg_metrics_dict)
                self.history["best_params"].append(best_params)
                self.history["generation_times"].append(time.time() - gen_start)
                
                # Update live display
                live.update(self.dashboard.render())
                
                # Last generation - skip evolution
                if gen == self.config.n_generations:
                    break
                
                # Select next generation
                offspring = self.toolbox.select(population, len(population))
                offspring = list(map(self.toolbox.clone, offspring))
                
                # Apply crossover
                for child1, child2 in zip(offspring[::2], offspring[1::2]):
                    if random.random() < self.config.crossover_prob:
                        self.toolbox.mate(child1, child2)
                        del child1.fitness.values
                        del child2.fitness.values
                
                # Apply mutation
                for mutant in offspring:
                    if random.random() < self.config.mutation_prob:
                        self.toolbox.mutate(mutant)
                        del mutant.fitness.values
                
                # Elitism - preserve best individuals
                if self.config.elite_size > 0:
                    elites = tools.selBest(population, self.config.elite_size)
                    offspring[:self.config.elite_size] = elites
                
                # Replace population
                population[:] = offspring
        
        # Final best (outside Live context)
        final_best = tools.selBest(population, 1)[0]
        self.best_individual = final_best
        self.best_params = self._individual_to_params(final_best)
        
        # Use the fitness from the final generation
        best_fit_tuple = final_best.fitness.values
        self.best_fitness = FitnessMetrics(
            sharpe_ratio=best_fit_tuple[0],
            win_rate=best_fit_tuple[1],
            max_drawdown_pct=best_fit_tuple[2],
            total_return=0.0,
            total_trades=0,
            profit_factor=0.0,
        )
        
        # Complete dashboard and show final
        self.dashboard.complete(self.history["best_fitness"][-1])
        console.print(self.dashboard.render())
        
        total_time = time.time() - start_time
        
        # Return results
        return {
            "best_params": self.best_params,
            "best_fitness": self.best_fitness.to_dict(),
            "history": self.history,
            "total_time": total_time,
            "total_evaluations": self.evaluator.eval_count,
            "config": self.config.model_dump(),
        }
