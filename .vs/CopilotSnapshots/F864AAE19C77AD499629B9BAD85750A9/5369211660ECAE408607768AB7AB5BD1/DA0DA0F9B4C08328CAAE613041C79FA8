"""
Backtest Engine

Professional backtesting system with position tracking, risk management, and performance metrics.
Includes advanced validation methods: Walk-Forward Analysis, K-Fold Cross-Validation, Monte Carlo Simulation.
"""

from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from typing import List, Optional, Dict, Any, Callable
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing as mp

import pandas as pd
import numpy as np

from ..strategies import BaseStrategy, SignalType
from ..core.logger import logger


class PositionSide(Enum):
    """Position side enum."""
    
    LONG = "LONG"
    SHORT = "SHORT"


@dataclass
class Position:
    """Active trading position."""
    
    side: PositionSide
    entry_price: float
    quantity: float
    entry_time: datetime
    stop_loss: Optional[float] = None
    take_profit: Optional[float] = None
    unrealized_pnl: float = 0.0
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class Trade:
    """Closed trade record."""
    
    side: PositionSide
    entry_price: float
    exit_price: float
    quantity: float
    entry_time: datetime
    exit_time: datetime
    pnl: float
    pnl_percent: float
    fees: float
    stop_loss: Optional[float] = None
    take_profit: Optional[float] = None
    exit_reason: str = "unknown"
    metadata: Dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> Dict[str, Any]:
        """Convert trade to dictionary."""
        return {
            "side": self.side.value,
            "entry_price": self.entry_price,
            "exit_price": self.exit_price,
            "quantity": self.quantity,
            "entry_time": str(self.entry_time),
            "exit_time": str(self.exit_time),
            "pnl": self.pnl,
            "pnl_percent": self.pnl_percent,
            "fees": self.fees,
            "stop_loss": self.stop_loss,
            "take_profit": self.take_profit,
            "exit_reason": self.exit_reason,
            "metadata": self.metadata,
        }


class BacktestEngine:
    """
    Backtest engine for strategy validation.
    
    Simulates live trading with proper position tracking, risk management,
    and performance metrics calculation.
    
    Advanced Features:
    - Walk-Forward Analysis (rolling out-of-sample validation)
    - K-Fold Cross-Validation
    - Monte Carlo Simulation
    - Parallel execution support
    """

    def __init__(
        self,
        initial_capital: float = 10000.0,
        commission_rate: float = 0.001,  # 0.1% per trade
        slippage_rate: float = 0.0005,  # 0.05% slippage
    ):
        """
        Initialize backtest engine.

        Args:
            initial_capital: Starting capital
            commission_rate: Commission as fraction (e.g., 0.001 = 0.1%)
            slippage_rate: Slippage as fraction
        """
        self.initial_capital = initial_capital
        self.commission_rate = commission_rate
        self.slippage_rate = slippage_rate

        # State
        self.cash = initial_capital
        self.equity = initial_capital
        self.position: Optional[Position] = None
        self.trades: List[Trade] = []
        self.equity_curve: List[tuple[datetime, float]] = []

        logger.info(
            f"Backtest engine initialized: capital=${initial_capital}, "
            f"commission={commission_rate*100}%, slippage={slippage_rate*100}%"
        )

    def run(
        self,
        strategy: BaseStrategy,
        df: pd.DataFrame,
    ) -> Dict[str, Any]:
        """
        Run backtest on historical data.

        Args:
            strategy: Trading strategy instance
            df: DataFrame with OHLCV and indicators

        Returns:
            Dictionary with backtest results
        """
        logger.info(f"Running backtest for {strategy.name}")

        # Reset state
        self._reset()

        # Generate signals
        df_with_signals = strategy.backtest_signals(df)

        # Simulate trading
        for idx in range(len(df_with_signals)):
            row = df_with_signals.iloc[idx]
            self._process_bar(row)

        # Close any open position at end
        if self.position:
            last_row = df_with_signals.iloc[-1]
            self._close_position(
                last_row["timestamp"],
                last_row["close"],
                "end_of_data",
            )

        # Calculate metrics
        metrics = self._calculate_metrics()

        logger.info(
            f"Backtest complete: {len(self.trades)} trades, "
            f"Final equity: ${self.equity:.2f}"
        )

        return {
            "strategy": strategy.name,
            "initial_capital": self.initial_capital,
            "final_equity": self.equity,
            "total_return": (self.equity / self.initial_capital - 1) * 100,
            "total_trades": len(self.trades),
            "metrics": metrics,
            "trades": [t.to_dict() for t in self.trades],
            "equity_curve": [
                {"timestamp": str(ts), "equity": eq}
                for ts, eq in self.equity_curve
            ],
        }

    def walk_forward_analysis(
        self,
        strategy: BaseStrategy,
        df: pd.DataFrame,
        train_days: int = 180,
        test_days: int = 60,
        step_days: int = 30,
        optimize_func: Optional[Callable] = None,
        parallel: bool = True,
        n_jobs: int = -1,
    ) -> Dict[str, Any]:
        """
        Perform Walk-Forward Analysis (rolling window validation).
        
        WFA divides data into multiple train/test windows and validates
        strategy performance out-of-sample. Critical for detecting overfitting.
        
        Args:
            strategy: Trading strategy to validate
            df: Full historical data
            train_days: Training window size in days
            test_days: Testing window size in days
            step_days: Step size for rolling window (< test_days for overlap)
            optimize_func: Optional function to optimize params on train data
            parallel: Execute windows in parallel
            n_jobs: Number of parallel workers (-1 = all CPUs)
            
        Returns:
            Dictionary with WFA results including:
            - windows: List of train/test results per window
            - stability_ratio: out_of_sample / in_sample return
            - consistency: % of profitable out-sample windows
            - avg_in_sample_return: Average training return
            - avg_out_sample_return: Average testing return
            - recommendation: PASS/FAIL based on thresholds
            
        Example:
            >>> engine = BacktestEngine()
            >>> results = engine.walk_forward_analysis(
            ...     strategy=my_strategy,
            ...     df=data,
            ...     train_days=180,
            ...     test_days=60,
            ...     step_days=30
            ... )
            >>> print(f"Stability Ratio: {results['stability_ratio']:.2f}")
        """
        logger.info(
            f"Starting Walk-Forward Analysis: {strategy.name}, "
            f"train={train_days}d, test={test_days}d, step={step_days}d"
        )
        
        # Validate data size
        min_required_days = train_days + test_days
        actual_days = (df['timestamp'].iloc[-1] - df['timestamp'].iloc[0]).days
        
        if actual_days < min_required_days:
            raise ValueError(
                f"Insufficient data: need {min_required_days} days, "
                f"got {actual_days} days"
            )
        
        # Create rolling windows
        windows = self._create_wfa_windows(df, train_days, test_days, step_days)
        
        logger.info(f"Created {len(windows)} WFA windows")
        
        # Execute windows (parallel or serial)
        if parallel and len(windows) > 1:
            window_results = self._execute_wfa_parallel(
                strategy, windows, optimize_func, n_jobs
            )
        else:
            window_results = self._execute_wfa_serial(
                strategy, windows, optimize_func
            )
        
        # Calculate aggregate metrics
        analysis = self._analyze_wfa_results(window_results)
        
        logger.info(
            f"WFA complete: Stability={analysis['stability_ratio']:.2f}, "
            f"Consistency={analysis['consistency']:.1f}%, "
            f"Recommendation={analysis['recommendation']}"
        )
        
        return analysis

    def k_fold_validation(
        self,
        strategy: BaseStrategy,
        df: pd.DataFrame,
        k: int = 5,
        shuffle: bool = False,
        parallel: bool = True,
        n_jobs: int = -1,
    ) -> Dict[str, Any]:
        """
        Perform K-Fold Cross-Validation.
        
        Divides data into K equal parts (folds), trains on K-1 folds and
        tests on the remaining fold. Repeats K times with different test folds.
        
        Complementary to Walk-Forward Analysis. While WFA preserves time order,
        K-Fold tests robustness across different data periods.
        
        Args:
            strategy: Trading strategy to validate
            df: Historical data
            k: Number of folds (typically 5 or 10)
            shuffle: Shuffle data before splitting (not recommended for time series)
            parallel: Execute folds in parallel
            n_jobs: Number of parallel workers (-1 = all CPUs)
            
        Returns:
            Dictionary with K-Fold results:
            - folds: List of individual fold results
            - mean_return: Average return across folds
            - std_return: Standard deviation of returns
            - consistency: % of profitable folds
            - recommendation: PASS/FAIL based on metrics
            
        Note:
            For time series (trading), shuffle=False preserves temporal ordering.
            Each fold tests on a different contiguous time period.
        """
        logger.info(
            f"Starting K-Fold Validation: {strategy.name}, "
            f"k={k}, shuffle={shuffle}"
        )
        
        # Create folds
        folds = self._create_k_folds(df, k, shuffle)
        
        logger.info(f"Created {k} folds")
        
        # Execute folds
        if parallel and k > 1:
            fold_results = self._execute_kfold_parallel(
                strategy, folds, n_jobs
            )
        else:
            fold_results = self._execute_kfold_serial(strategy, folds)
        
        # Analyze results
        analysis = self._analyze_kfold_results(fold_results)
        
        logger.info(
            f"K-Fold complete: Mean={analysis['mean_return']:.2f}%, "
            f"Std={analysis['std_return']:.2f}%, "
            f"Consistency={analysis['consistency']:.1f}%"
        )
        
        return analysis

    def monte_carlo_simulation(
        self,
        trades: List[Trade],
        n_simulations: int = 1000,
        n_trades: Optional[int] = None,
        parallel: bool = True,
        n_jobs: int = -1,
    ) -> Dict[str, Any]:
        """
        Perform Monte Carlo Simulation on trade sequences.
        
        Randomly resamples historical trades to generate thousands of possible
        equity curves. Helps understand risk and confidence intervals.
        
        Args:
            trades: Historical trades from backtest
            n_simulations: Number of random simulations (1000-10000)
            n_trades: Number of trades per simulation (None = use all)
            parallel: Execute simulations in parallel
            n_jobs: Number of parallel workers (-1 = all CPUs)
            
        Returns:
            Dictionary with Monte Carlo results:
            - median_return: Median simulated return
            - confidence_intervals: 95%, 90%, 75% intervals
            - risk_of_ruin: Probability of >20% drawdown
            - simulated_returns: Array of all simulated returns
            - equity_curves_sample: Sample curves for visualization
            
        Example:
            >>> # First run a backtest
            >>> results = engine.run(strategy, df)
            >>> 
            >>> # Then analyze risk with Monte Carlo
            >>> mc_results = engine.monte_carlo_simulation(
            ...     trades=results['trades'],
            ...     n_simulations=10000
            ... )
            >>> print(f"95% Confidence: {mc_results['confidence_intervals']['95']}")
        """
        logger.info(
            f"Starting Monte Carlo Simulation: "
            f"n_simulations={n_simulations}, n_trades={len(trades)}"
        )
        
        if not trades:
            raise ValueError("No trades provided for Monte Carlo simulation")
        
        # Run simulations
        if parallel:
            simulated_results = self._run_monte_carlo_parallel(
                trades, n_simulations, n_trades, n_jobs
            )
        else:
            simulated_results = self._run_monte_carlo_serial(
                trades, n_simulations, n_trades
            )
        
        # Analyze distribution
        analysis = self._analyze_monte_carlo_results(simulated_results, trades)
        
        logger.info(
            f"Monte Carlo complete: Median={analysis['median_return']:.2f}%, "
            f"95% CI=[{analysis['confidence_intervals']['95'][0]:.2f}%, "
            f"{analysis['confidence_intervals']['95'][1]:.2f}%]"
        )
        
        return analysis

    def _create_wfa_windows(
        self,
        df: pd.DataFrame,
        train_days: int,
        test_days: int,
        step_days: int,
    ) -> List[Dict[str, Any]]:
        """Create rolling train/test windows for WFA."""
        windows = []
        window_id = 1
        
        start_date = df['timestamp'].iloc[0]
        end_date = df['timestamp'].iloc[-1]
        
        current_start = start_date
        
        while True:
            train_start = current_start
            train_end = train_start + timedelta(days=train_days)
            test_start = train_end
            test_end = test_start + timedelta(days=test_days)
            
            # Check if we have enough data for this window
            if test_end > end_date:
                break
            
            # Extract data for this window
            train_mask = (df['timestamp'] >= train_start) & (df['timestamp'] < train_end)
            test_mask = (df['timestamp'] >= test_start) & (df['timestamp'] < test_end)
            
            train_df = df[train_mask].copy()
            test_df = df[test_mask].copy()
            
            # Skip if insufficient data
            if len(train_df) < 100 or len(test_df) < 20:
                logger.warning(f"Window {window_id}: insufficient data, skipping")
                current_start += timedelta(days=step_days)
                window_id += 1
                continue
            
            windows.append({
                'window_id': window_id,
                'train_start': train_start,
                'train_end': train_end,
                'test_start': test_start,
                'test_end': test_end,
                'train_df': train_df,
                'test_df': test_df,
            })
            
            # Move to next window
            current_start += timedelta(days=step_days)
            window_id += 1
        
        return windows

    def _execute_wfa_serial(
        self,
        strategy: BaseStrategy,
        windows: List[Dict[str, Any]],
        optimize_func: Optional[Callable],
    ) -> List[Dict[str, Any]]:
        """Execute WFA windows serially (single-threaded)."""
        results = []
        
        for window in windows:
            result = self._execute_single_window(strategy, window, optimize_func)
            results.append(result)
            
            logger.info(
                f"Window {window['window_id']}/{len(windows)}: "
                f"In-sample={result['in_sample_return']:.2f}%, "
                f"Out-sample={result['out_sample_return']:.2f}%"
            )
        
        return results

    def _execute_wfa_parallel(
        self,
        strategy: BaseStrategy,
        windows: List[Dict[str, Any]],
        optimize_func: Optional[Callable],
        n_jobs: int,
    ) -> List[Dict[str, Any]]:
        """Execute WFA windows in parallel (multi-process)."""
        if n_jobs == -1:
            n_jobs = mp.cpu_count()
        
        n_jobs = min(n_jobs, len(windows))  # Don't use more workers than windows
        
        logger.info(f"Executing {len(windows)} windows in parallel using {n_jobs} workers")
        
        results = []
        
        with ProcessPoolExecutor(max_workers=n_jobs) as executor:
            # Submit all windows
            future_to_window = {
                executor.submit(
                    self._execute_single_window_static,
                    strategy,
                    window,
                    optimize_func,
                    self.initial_capital,
                    self.commission_rate,
                    self.slippage_rate,
                ): window
                for window in windows
            }
            
            # Collect results as they complete
            for future in as_completed(future_to_window):
                window = future_to_window[future]
                try:
                    result = future.result()
                    results.append(result)
                    
                    logger.info(
                        f"Window {window['window_id']}/{len(windows)} complete: "
                        f"In={result['in_sample_return']:.2f}%, "
                        f"Out={result['out_sample_return']:.2f}%"
                    )
                except Exception as e:
                    logger.error(
                        f"Window {window['window_id']} failed: {e}",
                        exc_info=True
                    )
        
        # Sort by window_id
        results.sort(key=lambda x: x['window_id'])
        
        return results

    @staticmethod
    def _execute_single_window_static(
        strategy: BaseStrategy,
        window: Dict[str, Any],
        optimize_func: Optional[Callable],
        initial_capital: float,
        commission_rate: float,
        slippage_rate: float,
    ) -> Dict[str, Any]:
        """
        Static method for parallel execution (must be picklable).
        
        Note: Instance methods can't be pickled for multiprocessing,
        so we use a static method instead.
        """
        # Create fresh engine for this window
        engine = BacktestEngine(
            initial_capital=initial_capital,
            commission_rate=commission_rate,
            slippage_rate=slippage_rate,
        )
        
        return engine._execute_single_window(strategy, window, optimize_func)

    def _execute_single_window(
        self,
        strategy: BaseStrategy,
        window: Dict[str, Any],
        optimize_func: Optional[Callable],
    ) -> Dict[str, Any]:
        """Execute a single WFA window (train + test)."""
        # Train phase
        if optimize_func:
            # Optimize parameters on training data
            optimized_strategy = optimize_func(strategy, window['train_df'])
        else:
            optimized_strategy = strategy
        
        # Backtest on training data (in-sample)
        train_results = self.run(optimized_strategy, window['train_df'])
        
        # Backtest on testing data (out-of-sample)
        test_results = self.run(optimized_strategy, window['test_df'])
        
        return {
            'window_id': window['window_id'],
            'train_start': str(window['train_start']),
            'train_end': str(window['train_end']),
            'test_start': str(window['test_start']),
            'test_end': str(window['test_end']),
            'in_sample_return': train_results['total_return'],
            'out_sample_return': test_results['total_return'],
            'in_sample_trades': train_results['total_trades'],
            'out_sample_trades': test_results['total_trades'],
            'in_sample_sharpe': train_results['metrics']['sharpe_ratio'],
            'out_sample_sharpe': test_results['metrics']['sharpe_ratio'],
            'in_sample_win_rate': train_results['metrics']['win_rate'],
            'out_sample_win_rate': test_results['metrics']['win_rate'],
        }

    def _analyze_wfa_results(
        self,
        window_results: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Analyze WFA results and generate metrics."""
        if not window_results:
            return {
                'error': 'No valid windows',
                'recommendation': 'FAIL'
            }
        
        # Extract metrics
        in_sample_returns = [w['in_sample_return'] for w in window_results]
        out_sample_returns = [w['out_sample_return'] for w in window_results]
        
        avg_in_sample = np.mean(in_sample_returns)
        avg_out_sample = np.mean(out_sample_returns)
        
        # Stability ratio (out-of-sample / in-sample)
        # Values close to 1.0 are ideal (no overfitting)
        # Values > 0.7 are acceptable
        if avg_in_sample > 0:
            stability_ratio = avg_out_sample / avg_in_sample
        else:
            stability_ratio = 0.0
        
        # Consistency: % of profitable out-of-sample windows
        profitable_windows = sum(1 for r in out_sample_returns if r > 0)
        consistency = (profitable_windows / len(out_sample_returns)) * 100
        
        # Recommendation logic
        if stability_ratio >= 0.7 and consistency >= 70 and avg_out_sample > 0:
            recommendation = "PASS - Strategy validated, ready for optimization"
        elif stability_ratio >= 0.5 and consistency >= 50:
            recommendation = "MARGINAL - Consider parameter tuning"
        else:
            recommendation = "FAIL - Likely overfitting, do not use in production"
        
        return {
            'windows': window_results,
            'n_windows': len(window_results),
            'stability_ratio': round(stability_ratio, 3),
            'consistency': round(consistency, 1),
            'avg_in_sample_return': round(avg_in_sample, 2),
            'avg_out_sample_return': round(avg_out_sample, 2),
            'std_in_sample_return': round(np.std(in_sample_returns), 2),
            'std_out_sample_return': round(np.std(out_sample_returns), 2),
            'best_window': max(window_results, key=lambda x: x['out_sample_return']),
            'worst_window': min(window_results, key=lambda x: x['out_sample_return']),
            'recommendation': recommendation,
        }

    def _create_k_folds(
        self,
        df: pd.DataFrame,
        k: int,
        shuffle: bool = False,
    ) -> List[Dict[str, Any]]:
        """
        Create K equally sized folds for cross-validation.
        
        Args:
            df: DataFrame with complete dataset
            k: Number of folds
            shuffle: Shuffle data before splitting (not recommended for time series)
            
        Returns:
            List of folds, each with train and test DataFrames
            
        Note:
            For time series data, shuffle=False is important to preserve temporal order.
            Each fold will be a contiguous time segment.
        """
        if shuffle:
            df = df.sample(frac=1, random_state=42).reset_index(drop=True)
        
        fold_size = len(df) // k
        folds = []
        
        for i in range(k):
            test_start = i * fold_size
            test_end = test_start + fold_size
            
            if i == k - 1:
                # Last fold gets the remainder of the data
                test_end = len(df)
            
            test_df = df.iloc[test_start:test_end]
            train_df = pd.concat([df.iloc[:test_start], df.iloc[test_end:]])
            
            folds.append({
                'fold_id': i + 1,
                'train_df': train_df,
                'test_df': test_df,
            })
        
        return folds

    def _execute_kfold_serial(
        self,
        strategy: BaseStrategy,
        folds: List[Dict[str, Any]],
    ) -> List[Dict[str, Any]]:
        """Execute K-Fold validation serially (single-threaded)."""
        results = []
        
        for fold in folds:
            result = self._execute_single_fold(strategy, fold)
            results.append(result)
            
            logger.info(
                f"Fold {fold['fold_id']}/{len(folds)}: "
                f"Return={result['test_return']:.2f}%, "
                f"Profit Factor={result['profit_factor']:.2f}"
            )
        
        return results

    def _execute_kfold_parallel(
        self,
        strategy: BaseStrategy,
        folds: List[Dict[str, Any]],
        n_jobs: int,
    ) -> List[Dict[str, Any]]:
        """Execute K-Fold validation in parallel (multi-process)."""
        if n_jobs == -1:
            n_jobs = mp.cpu_count()
        
        n_jobs = min(n_jobs, len(folds))  # Don't use more workers than folds
        
        logger.info(f"Executing {len(folds)} folds in parallel using {n_jobs} workers")
        
        results = []
        
        with ProcessPoolExecutor(max_workers=n_jobs) as executor:
            # Submit all folds
            future_to_fold = {
                executor.submit(
                    self._execute_single_fold_static,
                    strategy,
                    fold,
                    self.initial_capital,
                    self.commission_rate,
                    self.slippage_rate,
                ): fold
                for fold in folds
            }
            
            # Collect results as they complete
            for future in as_completed(future_to_fold):
                fold = future_to_fold[future]
                try:
                    result = future.result()
                    results.append(result)
                    
                    logger.info(
                        f"Fold {fold['fold_id']}/{len(folds)} complete: "
                        f"Return={result['test_return']:.2f}%, "
                        f"PF={result['profit_factor']:.2f}"
                    )
                except Exception as e:
                    logger.error(
                        f"Fold {fold['fold_id']} failed: {e}",
                        exc_info=True
                    )
        
        # Sort by fold_id
        results.sort(key=lambda x: x['fold_id'])
        
        return results

    @staticmethod
    def _execute_single_fold_static(
        strategy: BaseStrategy,
        fold: Dict[str, Any],
        initial_capital: float,
        commission_rate: float,
        slippage_rate: float,
    ) -> Dict[str, Any]:
        """
        Static method for parallel execution of a single fold (must be picklable).
        
        Note: Instance methods can't be pickled for multiprocessing,
        so we use a static method instead.
        """
        # Create fresh engine for this fold
        engine = BacktestEngine(
            initial_capital=initial_capital,
            commission_rate=commission_rate,
            slippage_rate=slippage_rate,
        )
        
        return engine._execute_single_fold(strategy, fold)

    def _execute_single_fold(
        self,
        strategy: BaseStrategy,
        fold: Dict[str, Any],
    ) -> Dict[str, Any]:
        """Execute a single fold (train + test)."""
        # Train on full train data
        train_results = self.run(strategy, fold['train_df'])
        
        # Test on this fold's test data
        test_results = self.run(strategy, fold['test_df'])
        
        return {
            'fold_id': fold['fold_id'],
            'train_start': str(fold['train_df']['timestamp'].iloc[0]),
            'train_end': str(fold['train_df']['timestamp'].iloc[-1]),
            'test_start': str(fold['test_df']['timestamp'].iloc[0]),
            'test_end': str(fold['test_df']['timestamp'].iloc[-1]),
            'train_return': train_results['total_return'],
            'test_return': test_results['total_return'],
            'train_trades': train_results['total_trades'],
            'test_trades': test_results['total_trades'],
            'profit_factor': test_results['metrics']['profit_factor'],
            'sharpe_ratio': test_results['metrics']['sharpe_ratio'],
            'win_rate': test_results['metrics']['win_rate'],
        }

    def _run_monte_carlo_serial(
        self,
        trades: List[Trade],
        n_simulations: int,
        n_trades: Optional[int],
    ) -> List[Dict[str, Any]]:
        """Run Monte Carlo simulations serially (single-threaded)."""
        simulated_results = []
        
        # Use all trades if n_trades is not specified
        if n_trades is None:
            n_trades = len(trades)
        
        for i in range(n_simulations):
            # Resample trades with replacement
            sampled_trades = np.random.choice(trades, size=n_trades, replace=True)
            
            # Calculate total return for this simulation
            total_return = sum(t.pnl for t in sampled_trades)
            
            simulated_results.append({
                'simulation_id': i + 1,
                'total_return': total_return,
                'n_trades': n_trades,
                'trades': sampled_trades,
            })
        
        return simulated_results

    def _run_monte_carlo_parallel(
        self,
        trades: List[Trade],
        n_simulations: int,
        n_trades: Optional[int],
        n_jobs: int,
    ) -> List[Dict[str, Any]]:
        """Run Monte Carlo simulations in parallel (multi-process)."""
        if n_jobs == -1:
            n_jobs = mp.cpu_count()
        
        logger.info(f"Executing Monte Carlo with {n_simulations} simulations in parallel using {n_jobs} workers")
        
        with ProcessPoolExecutor(max_workers=n_jobs) as executor:
            # Submit all simulations
            futures = {
                executor.submit(
                    self._run_single_monte_carlo_simulation,
                    trades,
                    n_trades,
                ): i
                for i in range(n_simulations)
            }
            
            # Collect results as they complete
            results = []
            
            for future in as_completed(futures):
                simulation_id = futures[future]
                try:
                    result = future.result()
                    results.append(result)
                    
                    logger.info(
                        f"Simulation {simulation_id + 1}/{n_simulations} complete: "
                        f"Return={result['total_return']:.2f}"
                    )
                except Exception as e:
                    logger.error(
                        f"Simulation {simulation_id + 1} failed: {e}",
                        exc_info=True
                    )
        
        return results

    def _run_single_monte_carlo_simulation(
        self,
        trades: List[Trade],
        n_trades: Optional[int],
    ) -> Dict[str, Any]:
        """Run a single Monte Carlo simulation (worker function)."""
        # Use all trades if n_trades is not specified
        if n_trades is None:
            n_trades = len(trades)
        
        # Resample trades with replacement
        sampled_trades = np.random.choice(trades, size=n_trades, replace=True)
        
        # Calculate total return for this simulation
        total_return = sum(t.pnl for t in sampled_trades)
        
        return {
            'total_return': total_return,
            'n_trades': n_trades,
            'trades': sampled_trades,
        }

    def _analyze_kfold_results(
        self,
        fold_results: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Analyze K-Fold results and generate metrics."""
        if not fold_results:
            return {
                'error': 'No valid folds',
                'recommendation': 'FAIL'
            }
        
        # Extract metrics
        test_returns = [f['test_return'] for f in fold_results]
        
        avg_return = np.mean(test_returns)
        std_return = np.std(test_returns)
        
        # Consistency: % of profitable folds
        profitable_folds = sum(1 for r in test_returns if r > 0)
        consistency = (profitable_folds / len(test_returns)) * 100
        
        # Recommendation logic
        if consistency >= 70 and avg_return > 0:
            recommendation = "PASS - Strategy validated across folds"
        elif consistency >= 50:
            recommendation = "MARGINAL - Inconsistent performance"
        else:
            recommendation = "FAIL - Likely overfitting or poor robustness"
        
        return {
            'folds': fold_results,
            'n_folds': len(fold_results),
            'mean_return': round(avg_return, 2),
            'std_return': round(std_return, 2),
            'consistency': round(consistency, 1),
            'best_fold': max(fold_results, key=lambda x: x['test_return']),
            'worst_fold': min(fold_results, key=lambda x: x['test_return']),
            'recommendation': recommendation,
        }

    def _analyze_monte_carlo_results(
        self,
        simulated_results: List[Dict[str, Any]],
        historical_trades: List[Trade],
    ) -> Dict[str, Any]:
        """
        Analyze Monte Carlo simulation results.
        
        Calculates percentiles, risk of ruin, and other metrics from simulated returns.
        
        Args:
            simulated_results: List of dictionaries with simulation results
            historical_trades: Original historical trades (for context)
            
        Returns:
            Dictionary with analysis results:
            - median_return: Median of simulated returns
            - confidence_intervals: 95%, 90%, 75% confidence intervals
            - risk_of_ruin: Probability of hitting a -20% drawdown
            - simulated_returns: Array of all simulated returns
            - equity_curves_sample: Sample equity curves for visualization
        """
        if not simulated_results:
            return {
                'error': 'No simulated results',
                'risk_of_ruin': 0.0,
                'confidence_intervals': {
                    '95': (0.0, 0.0),
                    '90': (0.0, 0.0),
                    '75': (0.0, 0.0),
                },
            }
        
        # Extract all simulated returns
        all_simulated_returns = np.array([r['total_return'] for r in simulated_results])
        
        # Calculate percentiles
        percentiles = np.percentile(all_simulated_returns, [50, 75, 90, 95])
        
        # Risk of ruin: Probability of >20% drawdown
        # Drawdown is calculated as the change from the maximum equity point
        equity_curves = [r['trades'] for r in simulated_results]
        max_drawdowns = [
            np.min(np.array([t.pnl for t in curve]) - np.maximum.accumulate(np.array([t.pnl for t in curve]))) 
            for curve in equity_curves
        ]
        risk_of_ruin = np.mean(np.array(max_drawdowns) < -0.2) * 100  # % of simulations

        return {
            'median_return': percentiles[0],
            'confidence_intervals': {
                '95': (percentiles[2], percentiles[3]),
                '90': (percentiles[1], percentiles[2]),
                '75': (percentiles[0], percentiles[1]),
            },
            'risk_of_ruin': risk_of_ruin,
            'simulated_returns': all_simulated_returns,
            'equity_curves_sample': equity_curves[:5],  # Sample first 5 curves
        }

    def _reset(self) -> None:
        """Reset backtest state."""
        self.cash = self.initial_capital
        self.equity = self.initial_capital
        self.position = None
        self.trades = []
        self.equity_curve = []

    def _process_bar(self, row: pd.Series) -> None:
        """
        Process a single bar (candle).

        Args:
            row: DataFrame row with OHLCV and signal data
        """
        timestamp = row["timestamp"]
        high = row["high"]
        low = row["low"]
        close = row["close"]

        # Check for stop loss / take profit hits
        if self.position:
            self._check_exits(timestamp, high, low, close)

        # Check for new signals
        signal = row.get("signal", SignalType.HOLD.value)

        if signal == SignalType.LONG.value and not self.position:
            self._open_position(
                PositionSide.LONG,
                timestamp,
                row["signal_price"],
                row.get("stop_loss"),
                row.get("take_profit"),
            )
        elif signal == SignalType.SHORT.value and not self.position:
            self._open_position(
                PositionSide.SHORT,
                timestamp,
                row["signal_price"],
                row.get("stop_loss"),
                row.get("take_profit"),
            )
        elif signal == SignalType.CLOSE_LONG.value and self.position:
            if self.position.side == PositionSide.LONG:
                self._close_position(timestamp, close, "signal_exit")
        elif signal == SignalType.CLOSE_SHORT.value and self.position:
            if self.position.side == PositionSide.SHORT:
                self._close_position(timestamp, close, "signal_exit")

        # Update equity
        self._update_equity(timestamp, close)

    def _open_position(
        self,
        side: PositionSide,
        timestamp: datetime,
        price: float,
        stop_loss: Optional[float],
        take_profit: Optional[float],
    ) -> None:
        """Open a new position."""
        # Apply slippage
        entry_price = price * (1 + self.slippage_rate) if side == PositionSide.LONG else price * (1 - self.slippage_rate)

        # Calculate position size - Use fixed percentage of initial capital
        position_value = self.initial_capital * 0.10  # 10% of capital per trade (realistic)
        quantity = position_value / entry_price

        # Calculate entry fees
        entry_fees = quantity * entry_price * self.commission_rate

        self.position = Position(
            side=side,
            entry_price=entry_price,
            quantity=quantity,
            entry_time=timestamp,
            stop_loss=stop_loss,
            take_profit=take_profit,
        )

        # Deduct position value from cash (fees will be deducted on close from P&L)
        self.cash -= position_value

        logger.debug(
            f"Opened {side.value} position: qty={quantity:.4f}, "
            f"price=${entry_price:.2f}, value=${position_value:.2f}"
        )

    def _close_position(
        self,
        timestamp: datetime,
        price: float,
        reason: str,
    ) -> None:
        """Close the current position."""
        if not self.position:
            return

        # Apply slippage
        exit_price = price * (1 - self.slippage_rate) if self.position.side == PositionSide.LONG else price * (1 + self.slippage_rate)

        # Calculate raw P&L (before fees)
        if self.position.side == PositionSide.LONG:
            raw_pnl = (exit_price - self.position.entry_price) * self.position.quantity
        else:  # SHORT
            raw_pnl = (self.position.entry_price - exit_price) * self.position.quantity

        # Calculate total fees (entry + exit)
        entry_fees = self.position.quantity * self.position.entry_price * self.commission_rate
        exit_fees = self.position.quantity * exit_price * self.commission_rate
        total_fees = entry_fees + exit_fees

        # Net P&L after fees
        net_pnl = raw_pnl - total_fees

        # Return to cash: original position value + net P&L
        position_initial_value = self.position.entry_price * self.position.quantity
        self.cash += position_initial_value + net_pnl

        # Calculate P&L percentage
        pnl_percent = (raw_pnl / position_initial_value) * 100

        # Record trade
        trade = Trade(
            side=self.position.side,
            entry_price=self.position.entry_price,
            exit_price=exit_price,
            quantity=self.position.quantity,
            entry_time=self.position.entry_time,
            exit_time=timestamp,
            pnl=net_pnl,  # Net P&L after all fees
            pnl_percent=pnl_percent,
            fees=total_fees,
            stop_loss=self.position.stop_loss,
            take_profit=self.position.take_profit,
            exit_reason=reason,
        )

        self.trades.append(trade)
        self.position = None

        logger.debug(
            f"Closed position: pnl=${net_pnl:.2f} ({pnl_percent:.2f}%), "
            f"reason={reason}"
        )

    def _check_exits(
        self,
        timestamp: datetime,
        high: float,
        low: float,
        close: float,
    ) -> None:
        """Check if stop loss or take profit was hit."""
        if not self.position:
            return

        if self.position.side == PositionSide.LONG:
            # Check stop loss
            if self.position.stop_loss and low <= self.position.stop_loss:
                self._close_position(timestamp, self.position.stop_loss, "stop_loss")
                return

            # Check take profit
            if self.position.take_profit and high >= self.position.take_profit:
                self._close_position(timestamp, self.position.take_profit, "take_profit")
                return

        else:  # SHORT
            # Check stop loss
            if self.position.stop_loss and high >= self.position.stop_loss:
                self._close_position(timestamp, self.position.stop_loss, "stop_loss")
                return

            # Check take profit
            if self.position.take_profit and low <= self.position.take_profit:
                self._close_position(timestamp, self.position.take_profit, "take_profit")
                return

    def _update_equity(self, timestamp: datetime, price: float) -> None:
        """Update equity curve."""
        equity = self.cash

        if self.position:
            if self.position.side == PositionSide.LONG:
                unrealized = (price - self.position.entry_price) * self.position.quantity
            else:  # SHORT
                unrealized = (self.position.entry_price - price) * self.position.quantity

            equity += unrealized

        self.equity = equity
        self.equity_curve.append((timestamp, equity))

    def _calculate_metrics(self) -> Dict[str, Any]:
        """Calculate performance metrics."""
        if not self.trades:
            return {
                "total_trades": 0,
                "win_rate": 0.0,
                "avg_win": 0.0,
                "avg_loss": 0.0,
                "profit_factor": 0.0,
                "sharpe_ratio": 0.0,
                "max_drawdown": 0.0,
                "max_drawdown_pct": 0.0,
            }

        # Basic stats
        total_trades = len(self.trades)
        winning_trades = [t for t in self.trades if t.pnl > 0]
        losing_trades = [t for t in self.trades if t.pnl < 0]

        win_rate = len(winning_trades) / total_trades if total_trades > 0 else 0
        avg_win = np.mean([t.pnl for t in winning_trades]) if winning_trades else 0
        avg_loss = np.mean([t.pnl for t in losing_trades]) if losing_trades else 0

        # Profit factor
        total_wins = sum(t.pnl for t in winning_trades)
        total_losses = abs(sum(t.pnl for t in losing_trades))
        profit_factor = total_wins / total_losses if total_losses > 0 else 0

        # Sharpe ratio (simplified - assumes daily returns)
        if len(self.equity_curve) > 1:
            equity_values = np.array([eq for _, eq in self.equity_curve])
            returns = np.diff(equity_values) / equity_values[:-1]
            sharpe = np.sqrt(365) * np.mean(returns) / (np.std(returns) + 1e-9)
        else:
            sharpe = 0.0

        # Max drawdown
        equity_values = np.array([eq for _, eq in self.equity_curve])
        running_max = np.maximum.accumulate(equity_values)
        drawdown = equity_values - running_max
        max_dd = np.min(drawdown)
        max_dd_pct = (max_dd / running_max[np.argmin(drawdown)] * 100) if len(running_max) > 0 else 0

        return {
            "total_trades": total_trades,
            "winning_trades": len(winning_trades),
            "losing_trades": len(losing_trades),
            "win_rate": win_rate * 100,
            "avg_win": avg_win,
            "avg_loss": avg_loss,
            "profit_factor": profit_factor,
            "sharpe_ratio": sharpe,
            "max_drawdown": max_dd,
            "max_drawdown_pct": max_dd_pct,
        }


__all__ = ["BacktestEngine", "Position", "Trade", "PositionSide"]
