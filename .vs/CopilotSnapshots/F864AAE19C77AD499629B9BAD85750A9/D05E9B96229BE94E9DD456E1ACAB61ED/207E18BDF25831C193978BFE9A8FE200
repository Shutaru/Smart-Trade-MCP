"""
Backtest Engine

Professional backtesting system with position tracking, risk management, and performance metrics.
Includes advanced validation methods: Walk-Forward Analysis, K-Fold Cross-Validation, Monte Carlo Simulation.
"""

from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from typing import List, Optional, Dict, Any, Callable
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing as mp

import pandas as pd
import numpy as np

from ..strategies import BaseStrategy, SignalType
from ..core.logger import logger
from ..core.gpu_utils import (
    GPU_AVAILABLE,
    HAS_CUPY,
    cp,
    to_gpu,
    to_cpu,
    synchronize,
    get_gpu_info,
)


class PositionSide(Enum):
    """Position side enum."""
    
    LONG = "LONG"
    SHORT = "SHORT"


@dataclass
class Position:
    """Active trading position."""
    
    side: PositionSide
    entry_price: float
    quantity: float
    entry_time: datetime
    stop_loss: Optional[float] = None
    take_profit: Optional[float] = None
    unrealized_pnl: float = 0.0
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class Trade:
    """Closed trade record."""
    
    side: PositionSide
    entry_price: float
    exit_price: float
    quantity: float
    entry_time: datetime
    exit_time: datetime
    pnl: float
    pnl_percent: float
    fees: float
    stop_loss: Optional[float] = None
    take_profit: Optional[float] = None
    exit_reason: str = "unknown"
    metadata: Dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> Dict[str, Any]:
        """Convert trade to dictionary."""
        return {
            "side": self.side.value,
            "entry_price": self.entry_price,
            "exit_price": self.exit_price,
            "quantity": self.quantity,
            "entry_time": str(self.entry_time),
            "exit_time": str(self.exit_time),
            "pnl": self.pnl,
            "pnl_percent": self.pnl_percent,
            "fees": self.fees,
            "stop_loss": self.stop_loss,
            "take_profit": self.take_profit,
            "exit_reason": self.exit_reason,
            "metadata": self.metadata,
        }


class BacktestEngine:
    """
    Backtest engine for strategy validation.
    
    Simulates live trading with proper position tracking, risk management,
    and performance metrics calculation.
    
    Advanced Features:
    - Walk-Forward Analysis (rolling out-of-sample validation)
    - K-Fold Cross-Validation
    - Monte Carlo Simulation
    - Parallel execution support
    """

    def __init__(
        self,
        initial_capital: float = 10000.0,
        commission_rate: float = 0.001,  # 0.1% per trade
        slippage_rate: float = 0.0005,  # 0.05% slippage
        use_gpu: bool = True,  # Enable GPU by default if available
    ):
        """
        Initialize backtest engine.

        Args:
            initial_capital: Starting capital
            commission_rate: Commission as fraction (e.g., 0.001 = 0.1%)
            slippage_rate: Slippage as fraction
            use_gpu: Use GPU acceleration if available (auto-fallback to CPU)
        """
        self.initial_capital = initial_capital
        self.commission_rate = commission_rate
        self.slippage_rate = slippage_rate
        self.use_gpu = use_gpu and GPU_AVAILABLE

        # State
        self.cash = initial_capital
        self.equity = initial_capital
        self.position: Optional[Position] = None
        self.trades: List[Trade] = []
        self.equity_curve: List[tuple[datetime, float]] = []

        # Log GPU status
        if self.use_gpu:
            gpu_info = get_gpu_info()
            logger.info(
                f"Backtest engine initialized with GPU: {gpu_info['device_name']} "
                f"(capital=${initial_capital})"
            )
        else:
            logger.info(
                f"Backtest engine initialized with CPU: capital=${initial_capital}, "
                f"commission={commission_rate*100}%, slippage={slippage_rate*100}%"
            )

    def run(
        self,
        strategy: BaseStrategy,
        df: pd.DataFrame,
    ) -> Dict[str, Any]:
        """
        Run backtest on historical data.

        Args:
            strategy: Trading strategy instance
            df: DataFrame with OHLCV and indicators

        Returns:
            Dictionary with backtest results
        """
        logger.info(f"Running backtest for {strategy.name}")

        # Reset state
        self._reset()

        # Generate signals
        df_with_signals = strategy.backtest_signals(df)

        # Simulate trading
        for idx in range(len(df_with_signals)):
            row = df_with_signals.iloc[idx]
            self._process_bar(row)

        # Close any open position at end
        if self.position:
            last_row = df_with_signals.iloc[-1]
            self._close_position(
                last_row["timestamp"],
                last_row["close"],
                "end_of_data",
            )

        # Calculate metrics
        metrics = self._calculate_metrics()

        logger.info(
            f"Backtest complete: {len(self.trades)} trades, "
            f"Final equity: ${self.equity:.2f}"
        )

        return {
            "strategy": strategy.name,
            "initial_capital": self.initial_capital,
            "final_equity": self.equity,
            "total_return": (self.equity / self.initial_capital - 1) * 100,
            "total_trades": len(self.trades),
            "metrics": metrics,
            "trades": [t.to_dict() for t in self.trades],
            "equity_curve": [
                {"timestamp": str(ts), "equity": eq}
                for ts, eq in self.equity_curve
            ],
        }

    def walk_forward_analysis(
        self,
        strategy: BaseStrategy,
        df: pd.DataFrame,
        train_days: int = 180,
        test_days: int = 60,
        step_days: int = 30,
        optimize_func: Optional[Callable] = None,
        parallel: bool = True,
        n_jobs: int = -1,
    ) -> Dict[str, Any]:
        """
        Perform Walk-Forward Analysis (rolling window validation).
        
        WFA divides data into multiple train/test windows and validates
        strategy performance out-of-sample. Critical for detecting overfitting.
        
        Args:
            strategy: Trading strategy to validate
            df: Full historical data
            train_days: Training window size in days
            test_days: Testing window size in days
            step_days: Step size for rolling window (< test_days for overlap)
            optimize_func: Optional function to optimize params on train data
            parallel: Execute windows in parallel
            n_jobs: Number of parallel workers (-1 = all CPUs)
            
        Returns:
            Dictionary with WFA results including:
            - windows: List of train/test results per window
            - stability_ratio: out_of_sample / in_sample return
            - consistency: % of profitable out-sample windows
            - avg_in_sample_return: Average training return
            - avg_out_sample_return: Average testing return
            - recommendation: PASS/FAIL based on thresholds
            
        Example:
            >>> engine = BacktestEngine()
            >>> results = engine.walk_forward_analysis(
            ...     strategy=my_strategy,
            ...     df=data,
            ...     train_days=180,
            ...     test_days=60,
            ...     step_days=30
            ... )
            >>> print(f"Stability Ratio: {results['stability_ratio']:.2f}")
        """
        logger.info(
            f"Starting Walk-Forward Analysis: {strategy.name}, "
            f"train={train_days}d, test={test_days}d, step={step_days}d"
        )
        
        # Validate data size
        min_required_days = train_days + test_days
        actual_days = (df['timestamp'].iloc[-1] - df['timestamp'].iloc[0]).days
        
        if actual_days < min_required_days:
            raise ValueError(
                f"Insufficient data: need {min_required_days} days, "
                f"got {actual_days} days"
            )
        
        # Create rolling windows
        windows = self._create_wfa_windows(df, train_days, test_days, step_days)
        
        logger.info(f"Created {len(windows)} WFA windows")
        
        # Execute windows (parallel or serial)
        if parallel and len(windows) > 1:
            window_results = self._execute_wfa_parallel(
                strategy, windows, optimize_func, n_jobs
            )
        else:
            window_results = self._execute_wfa_serial(
                strategy, windows, optimize_func
            )
        
        # Calculate aggregate metrics
        analysis = self._analyze_wfa_results(window_results)
        
        logger.info(
            f"WFA complete: Stability={analysis['stability_ratio']:.2f}, "
            f"Consistency={analysis['consistency']:.1f}%, "
            f"Recommendation={analysis['recommendation']}"
        )
        
        return analysis

    def k_fold_validation(
        self,
        strategy: BaseStrategy,
        df: pd.DataFrame,
        k: int = 5,
        shuffle: bool = False,
        parallel: bool = True,
        n_jobs: int = -1,
    ) -> Dict[str, Any]:
        """
        Perform K-Fold Cross-Validation.
        
        Divides data into K equal parts (folds), trains on K-1 folds and
        tests on the remaining fold. Repeats K times with different test folds.
        
        Complementary to Walk-Forward Analysis. While WFA preserves time order,
        K-Fold tests robustness across different data periods.
        
        Args:
            strategy: Trading strategy to validate
            df: Historical data
            k: Number of folds (typically 5 or 10)
            shuffle: Shuffle data before splitting (not recommended for time series)
            parallel: Execute folds in parallel
            n_jobs: Number of parallel workers (-1 = all CPUs)
            
        Returns:
            Dictionary with K-Fold results:
            - folds: List of individual fold results
            - mean_return: Average return across folds
            - std_return: Standard deviation of returns
            - consistency: % of profitable folds
            - recommendation: PASS/FAIL based on metrics
            
        Note:
            For time series (trading), shuffle=False preserves temporal ordering.
            Each fold tests on a different contiguous time period.
        """
        logger.info(
            f"Starting K-Fold Validation: {strategy.name}, "
            f"k={k}, shuffle={shuffle}"
        )
        
        # Create folds
        folds = self._create_k_folds(df, k, shuffle)
        
        logger.info(f"Created {k} folds")
        
        # Execute folds
        if parallel and k > 1:
            fold_results = self._execute_kfold_parallel(
                strategy, folds, n_jobs
            )
        else:
            fold_results = self._execute_kfold_serial(strategy, folds)
        
        # Analyze results
        analysis = self._analyze_kfold_results(fold_results)
        
        logger.info(
            f"K-Fold complete: Mean={analysis['mean_return']:.2f}%, "
            f"Std={analysis['std_return']:.2f}%, "
            f"Consistency={analysis['consistency']:.1f}%"
        )
        
        return analysis

    def monte_carlo_simulation(
        self,
        trades: List[Trade],
        n_simulations: int = 1000,
        n_trades: Optional[int] = None,
        parallel: bool = True,
        n_jobs: int = -1,
    ) -> Dict[str, Any]:
        """
        Perform Monte Carlo Simulation on trade sequences.
        
        Randomly resamples historical trades to generate thousands of possible
        equity curves. Helps understand risk and confidence intervals.
        
        Args:
            trades: Historical trades from backtest
            n_simulations: Number of random simulations (1000-10000)
            n_trades: Number of trades per simulation (None = use all)
            parallel: Execute simulations in parallel
            n_jobs: Number of parallel workers (-1 = all CPUs)
            
        Returns:
            Dictionary with Monte Carlo results:
            - median_return: Median simulated return
            - confidence_intervals: 95%, 90%, 75% intervals
            - risk_of_ruin: Probability of >20% drawdown
            - simulated_returns: Array of all simulated returns
            - equity_curves_sample: Sample curves for visualization
            
        Example:
            >>> # First run a backtest
            >>> results = engine.run(strategy, df)
            >>> 
            >>> # Then analyze risk with Monte Carlo
            >>> mc_results = engine.monte_carlo_simulation(
            ...     trades=results['trades'],
            ...     n_simulations=10000
            ... )
            >>> print(f"95% Confidence: {mc_results['confidence_intervals']['95']}")
        """
        logger.info(
            f"Starting Monte Carlo Simulation: "
            f"n_simulations={n_simulations}, n_trades={len(trades)}"
        )
        
        if not trades:
            raise ValueError("No trades provided for Monte Carlo simulation")
        
        # Run simulations
        if parallel:
            simulated_results = self._run_monte_carlo_parallel(
                trades, n_simulations, n_trades, n_jobs
            )
        else:
            simulated_results = self._run_monte_carlo_serial(
                trades, n_simulations, n_trades
            )
        
        # Analyze distribution
        analysis = self._analyze_monte_carlo_results(simulated_results, trades)
        
        logger.info(
            f"Monte Carlo complete: Median={analysis['median_return']:.2f}%, "
            f"95% CI=[{analysis['confidence_intervals']['95'][0]:.2f}%, "
            f"{analysis['confidence_intervals']['95'][1]:.2f}%]"
        )
        
        return analysis

    def _create_wfa_windows(
        self,
        df: pd.DataFrame,
        train_days: int,
        test_days: int,
        step_days: int,
    ) -> List[Dict[str, Any]]:
        """Create rolling train/test windows for WFA."""
        windows = []
        window_id = 1
        
        start_date = df['timestamp'].iloc[0]
        end_date = df['timestamp'].iloc[-1]
        
        current_start = start_date
        
        while True:
            train_start = current_start
            train_end = train_start + timedelta(days=train_days)
            test_start = train_end
            test_end = test_start + timedelta(days=test_days)
            
            # Check if we have enough data for this window
            if test_end > end_date:
                break
            
            # Extract data for this window
            train_mask = (df['timestamp'] >= train_start) & (df['timestamp'] < train_end)
            test_mask = (df['timestamp'] >= test_start) & (df['timestamp'] < test_end)
            
            train_df = df[train_mask].copy()
            test_df = df[test_mask].copy()
            
            # Skip if insufficient data
            if len(train_df) < 100 or len(test_df) < 20:
                logger.warning(f"Window {window_id}: insufficient data, skipping")
                current_start += timedelta(days=step_days)
                window_id += 1
                continue
            
            windows.append({
                'window_id': window_id,
                'train_start': train_start,
                'train_end': train_end,
                'test_start': test_start,
                'test_end': test_end,
                'train_df': train_df,
                'test_df': test_df,
            })
            
            # Move to next window
            current_start += timedelta(days=step_days)
            window_id += 1
        
        return windows

    def _execute_wfa_serial(
        self,
        strategy: BaseStrategy,
        windows: List[Dict[str, Any]],
        optimize_func: Optional[Callable],
    ) -> List[Dict[str, Any]]:
        """Execute WFA windows serially (single-threaded)."""
        results = []
        
        for window in windows:
            result = self._execute_single_window(strategy, window, optimize_func)
            results.append(result)
            
            logger.info(
                f"Window {window['window_id']}/{len(windows)}: "
                f"In-sample={result['in_sample_return']:.2f}%, "
                f"Out-sample={result['out_sample_return']:.2f}%"
            )
        
        return results

    def _execute_wfa_parallel(
        self,
        strategy: BaseStrategy,
        windows: List[Dict[str, Any]],
        optimize_func: Optional[Callable],
        n_jobs: int,
    ) -> List[Dict[str, Any]]:
        """Execute WFA windows in parallel (multi-process)."""
        if n_jobs == -1:
            n_jobs = mp.cpu_count()
        
        n_jobs = min(n_jobs, len(windows))  # Don't use more workers than windows
        
        logger.info(f"Executing {len(windows)} windows in parallel using {n_jobs} workers")
        
        results = []
        
        with ProcessPoolExecutor(max_workers=n_jobs) as executor:
            # Submit all windows
            future_to_window = {
                executor.submit(
                    self._execute_single_window_static,
                    strategy,
                    window,
                    optimize_func,
                    self.initial_capital,
                    self.commission_rate,
                    self.slippage_rate,
                ): window
                for window in windows
            }
            
            # Collect results as they complete
            for future in as_completed(future_to_window):
                window = future_to_window[future]
                try:
                    result = future.result()
                    results.append(result)
                    
                    logger.info(
                        f"Window {window['window_id']}/{len(windows)} complete: "
                        f"In={result['in_sample_return']:.2f}%, "
                        f"Out={result['out_sample_return']:.2f}%"
                    )
                except Exception as e:
                    logger.error(
                        f"Window {window['window_id']} failed: {e}",
                        exc_info=True
                    )
        
        # Sort by window_id
        results.sort(key=lambda x: x['window_id'])
        
        return results

    @staticmethod
    def _execute_single_window_static(
        strategy: BaseStrategy,
        window: Dict[str, Any],
        optimize_func: Optional[Callable],
        initial_capital: float,
        commission_rate: float,
        slippage_rate: float,
    ) -> Dict[str, Any]:
        """
        Static method for parallel execution (must be picklable).
        
        Note: Instance methods can't be pickled for multiprocessing,
        so we use a static method instead.
        """
        # Create fresh engine for this window
        engine = BacktestEngine(
            initial_capital=initial_capital,
            commission_rate=commission_rate,
            slippage_rate=slippage_rate,
        )
        
        return engine._execute_single_window(strategy, window, optimize_func)

    def _execute_single_window(
        self,
        strategy: BaseStrategy,
        window: Dict[str, Any],
        optimize_func: Optional[Callable],
    ) -> Dict[str, Any]:
        """Execute a single WFA window (train + test)."""
        # Train phase
        if optimize_func:
            # Optimize parameters on training data
            optimized_strategy = optimize_func(strategy, window['train_df'])
        else:
            optimized_strategy = strategy
        
        # Backtest on training data (in-sample)
        train_results = self.run(optimized_strategy, window['train_df'])
        
        # Backtest on testing data (out-of-sample)
        test_results = self.run(optimized_strategy, window['test_df'])
        
        return {
            'window_id': window['window_id'],
            'train_start': str(window['train_start']),
            'train_end': str(window['train_end']),
            'test_start': str(window['test_start']),
            'test_end': str(window['test_end']),
            'in_sample_return': train_results['total_return'],
            'out_sample_return': test_results['total_return'],
            'in_sample_trades': train_results['total_trades'],
            'out_sample_trades': test_results['total_trades'],
            'in_sample_sharpe': train_results['metrics']['sharpe_ratio'],
            'out_sample_sharpe': test_results['metrics']['sharpe_ratio'],
            'in_sample_win_rate': train_results['metrics']['win_rate'],
            'out_sample_win_rate': test_results['metrics']['win_rate'],
        }

    def _analyze_wfa_results(
        self,
        window_results: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Analyze WFA results and generate metrics."""
        if not window_results:
            return {
                'error': 'No valid windows',
                'recommendation': 'FAIL'
            }
        
        # Extract metrics
        in_sample_returns = [w['in_sample_return'] for w in window_results]
        out_sample_returns = [w['out_sample_return'] for w in window_results]
        
        avg_in_sample = np.mean(in_sample_returns)
        avg_out_sample = np.mean(out_sample_returns)
        
        # Stability ratio (out-of-sample / in-sample)
        # Values close to 1.0 are ideal (no overfitting)
        # Values > 0.7 are acceptable
        if avg_in_sample > 0:
            stability_ratio = avg_out_sample / avg_in_sample
        else:
            stability_ratio = 0.0
        
        # Consistency: % of profitable out-of-sample windows
        profitable_windows = sum(1 for r in out_sample_returns if r > 0)
        consistency = (profitable_windows / len(out_sample_returns)) * 100
        
        # Recommendation logic
        if stability_ratio >= 0.7 and consistency >= 70 and avg_out_sample > 0:
            recommendation = "PASS - Strategy validated, ready for optimization"
        elif stability_ratio >= 0.5 and consistency >= 50:
            recommendation = "MARGINAL - Consider parameter tuning"
        else:
            recommendation = "FAIL - Likely overfitting, do not use in production"
        
        return {
            'windows': window_results,
            'n_windows': len(window_results),
            'stability_ratio': round(stability_ratio, 3),
            'consistency': round(consistency, 1),
            'avg_in_sample_return': round(avg_in_sample, 2),
            'avg_out_sample_return': round(avg_out_sample, 2),
            'std_in_sample_return': round(np.std(in_sample_returns), 2),
            'std_out_sample_return': round(np.std(out_sample_returns), 2),
            'best_window': max(window_results, key=lambda x: x['out_sample_return']),
            'worst_window': min(window_results, key=lambda x: x['out_sample_return']),
            'recommendation': recommendation,
        }

    # K-Fold Validation Helper Methods

    def _create_k_folds(
        self,
        df: pd.DataFrame,
        k: int,
        shuffle: bool,
    ) -> List[Dict[str, Any]]:
        """Create K folds from data."""
        n_samples = len(df)
        fold_size = n_samples // k
        
        indices = np.arange(n_samples)
        if shuffle:
            np.random.shuffle(indices)
        
        folds = []
        for fold_id in range(k):
            # Test indices for this fold
            test_start = fold_id * fold_size
            test_end = (fold_id + 1) * fold_size if fold_id < k - 1 else n_samples
            test_indices = indices[test_start:test_end]
            
            # Train indices (all except test)
            train_indices = np.concatenate([
                indices[:test_start],
                indices[test_end:]
            ])
            
            folds.append({
                'fold_id': fold_id + 1,
                'train_df': df.iloc[train_indices].copy().sort_values('timestamp'),
                'test_df': df.iloc[test_indices].copy().sort_values('timestamp'),
            })
        
        return folds

    def _execute_kfold_serial(
        self,
        strategy: BaseStrategy,
        folds: List[Dict[str, Any]],
    ) -> List[Dict[str, Any]]:
        """Execute K-Fold serially."""
        results = []
        
        for fold in folds:
            # Run backtest on train and test
            train_results = self.run(strategy, fold['train_df'])
            test_results = self.run(strategy, fold['test_df'])
            
            results.append({
                'fold_id': fold['fold_id'],
                'train_size': len(fold['train_df']),
                'test_size': len(fold['test_df']),
                'train_return': train_results['total_return'],
                'test_return': test_results['total_return'],
                'train_trades': train_results['total_trades'],
                'test_trades': test_results['total_trades'],
                'test_sharpe': test_results['metrics']['sharpe_ratio'],
                'test_win_rate': test_results['metrics']['win_rate'],
            })
            
            logger.info(
                f"Fold {fold['fold_id']}/{len(folds)}: "
                f"Test return={results[-1]['test_return']:.2f}%"
            )
        
        return results

    def _execute_kfold_parallel(
        self,
        strategy: BaseStrategy,
        folds: List[Dict[str, Any]],
        n_jobs: int,
    ) -> List[Dict[str, Any]]:
        """Execute K-Fold in parallel."""
        if n_jobs == -1:
            n_jobs = mp.cpu_count()
        
        n_jobs = min(n_jobs, len(folds))
        
        logger.info(f"Executing {len(folds)} folds in parallel using {n_jobs} workers")
        
        results = []
        
        with ProcessPoolExecutor(max_workers=n_jobs) as executor:
            future_to_fold = {
                executor.submit(
                    self._execute_single_fold_static,
                    strategy,
                    fold,
                    self.initial_capital,
                    self.commission_rate,
                    self.slippage_rate,
                ): fold
                for fold in folds
            }
            
            for future in as_completed(future_to_fold):
                fold = future_to_fold[future]
                try:
                    result = future.result()
                    results.append(result)
                    
                    logger.info(
                        f"Fold {fold['fold_id']}/{len(folds)} complete: "
                        f"Test={result['test_return']:.2f}%"
                    )
                except Exception as e:
                    logger.error(f"Fold {fold['fold_id']} failed: {e}", exc_info=True)
        
        results.sort(key=lambda x: x['fold_id'])
        return results

    @staticmethod
    def _execute_single_fold_static(
        strategy: BaseStrategy,
        fold: Dict[str, Any],
        initial_capital: float,
        commission_rate: float,
        slippage_rate: float,
    ) -> Dict[str, Any]:
        """Static method for parallel K-Fold execution."""
        engine = BacktestEngine(
            initial_capital=initial_capital,
            commission_rate=commission_rate,
            slippage_rate=slippage_rate,
        )
        
        train_results = engine.run(strategy, fold['train_df'])
        test_results = engine.run(strategy, fold['test_df'])
        
        return {
            'fold_id': fold['fold_id'],
            'train_size': len(fold['train_df']),
            'test_size': len(fold['test_df']),
            'train_return': train_results['total_return'],
            'test_return': test_results['total_return'],
            'train_trades': train_results['total_trades'],
            'test_trades': test_results['total_trades'],
            'test_sharpe': test_results['metrics']['sharpe_ratio'],
            'test_win_rate': test_results['metrics']['win_rate'],
        }

    def _analyze_kfold_results(
        self,
        fold_results: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Analyze K-Fold results."""
        if not fold_results:
            return {
                'error': 'No valid folds',
                'recommendation': 'FAIL'
            }
        
        test_returns = [f['test_return'] for f in fold_results]
        
        mean_return = np.mean(test_returns)
        std_return = np.std(test_returns)
        
        profitable_folds = sum(1 for r in test_returns if r > 0)
        consistency = (profitable_folds / len(test_returns)) * 100
        
        # Recommendation
        if mean_return > 0 and consistency >= 60 and std_return < mean_return * 2:
            recommendation = "PASS - Consistent across folds"
        elif mean_return > 0 and consistency >= 40:
            recommendation = "MARGINAL - Some variability across folds"
        else:
            recommendation = "FAIL - Inconsistent or unprofitable"
        
        return {
            'folds': fold_results,
            'k': len(fold_results),
            'mean_return': round(mean_return, 2),
            'std_return': round(std_return, 2),
            'min_return': round(min(test_returns), 2),
            'max_return': round(max(test_returns), 2),
            'consistency': round(consistency, 1),
            'recommendation': recommendation,
        }

    # Monte Carlo Simulation Helper Methods

    def _run_monte_carlo_serial(
        self,
        trades: List[Trade],
        n_simulations: int,
        n_trades: Optional[int],
    ) -> List[float]:
        """Run Monte Carlo simulations serially."""
        if n_trades is None:
            n_trades = len(trades)
        
        simulated_returns = []
        
        for i in range(n_simulations):
            # Randomly resample trades with replacement
            sampled_trades = np.random.choice(trades, size=n_trades, replace=True)
            
            # Calculate cumulative return
            total_pnl = sum(t.pnl for t in sampled_trades)
            total_return = (total_pnl / self.initial_capital) * 100
            
            simulated_returns.append(total_return)
            
            if (i + 1) % 100 == 0:
                logger.debug(f"Monte Carlo: {i+1}/{n_simulations} simulations complete")
        
        return simulated_returns

    def _run_monte_carlo_parallel(
        self,
        trades: List[Trade],
        n_simulations: int,
        n_trades: Optional[int],
        n_jobs: int,
    ) -> List[float]:
        """Run Monte Carlo simulations in parallel."""
        if n_jobs == -1:
            n_jobs = mp.cpu_count()
        
        if n_trades is None:
            n_trades = len(trades)
        
        # Split simulations across workers
        sims_per_worker = n_simulations // n_jobs
        
        logger.info(
            f"Running {n_simulations} Monte Carlo simulations "
            f"in parallel using {n_jobs} workers"
        )
        
        all_returns = []
        
        with ProcessPoolExecutor(max_workers=n_jobs) as executor:
            futures = [
                executor.submit(
                    self._monte_carlo_worker,
                    trades,
                    sims_per_worker,
                    n_trades,
                    self.initial_capital,
                )
                for _ in range(n_jobs)
            ]
            
            for future in as_completed(futures):
                try:
                    returns = future.result()
                    all_returns.extend(returns)
                except Exception as e:
                    logger.error(f"Monte Carlo worker failed: {e}", exc_info=True)
        
        return all_returns

    @staticmethod
    def _monte_carlo_worker(
        trades: List[Trade],
        n_simulations: int,
        n_trades: int,
        initial_capital: float,
    ) -> List[float]:
        """Worker function for parallel Monte Carlo."""
        simulated_returns = []
        
        for _ in range(n_simulations):
            sampled_trades = np.random.choice(trades, size=n_trades, replace=True)
            total_pnl = sum(t.pnl for t in sampled_trades)
            total_return = (total_pnl / initial_capital) * 100
            simulated_returns.append(total_return)
        
        return simulated_returns

    def _analyze_monte_carlo_results(
        self,
        simulated_returns: List[float],
        original_trades: List[Trade],
    ) -> Dict[str, Any]:
        """Analyze Monte Carlo simulation results."""
        simulated_returns = np.array(simulated_returns)
        
        # Basic statistics
        median_return = np.median(simulated_returns)
        mean_return = np.mean(simulated_returns)
        std_return = np.std(simulated_returns)
        
        # Confidence intervals
        ci_95 = (np.percentile(simulated_returns, 2.5), np.percentile(simulated_returns, 97.5))
        ci_90 = (np.percentile(simulated_returns, 5), np.percentile(simulated_returns, 95))
        ci_75 = (np.percentile(simulated_returns, 12.5), np.percentile(simulated_returns, 87.5))
        
        # Risk of ruin (probability of >20% drawdown)
        # Simplified: count simulations with return < -20%
        risk_of_ruin = (np.sum(simulated_returns < -20) / len(simulated_returns)) * 100
        
        # Best and worst cases
        worst_case = np.percentile(simulated_returns, 1)
        best_case = np.percentile(simulated_returns, 99)
        
        return {
            'n_simulations': len(simulated_returns),
            'median_return': round(median_return, 2),
            'mean_return': round(mean_return, 2),
            'std_return': round(std_return, 2),
            'confidence_intervals': {
                '95': [round(ci_95[0], 2), round(ci_95[1], 2)],
                '90': [round(ci_90[0], 2), round(ci_90[1], 2)],
                '75': [round(ci_75[0], 2), round(ci_75[1], 2)],
            },
            'risk_of_ruin': round(risk_of_ruin, 2),
            'worst_case': round(worst_case, 2),
            'best_case': round(best_case, 2),
            'simulated_returns': simulated_returns.tolist(),  # For visualization
        }

    def _reset(self) -> None:
        """Reset backtest state."""
        self.cash = self.initial_capital
        self.equity = self.initial_capital
        self.position = None
        self.trades = []
        self.equity_curve = []

    def _process_bar(self, row: pd.Series) -> None:
        """
        Process a single bar (candle).

        Args:
            row: DataFrame row with OHLCV and signal data
        """
        timestamp = row["timestamp"]
        high = row["high"]
        low = row["low"]
        close = row["close"]

        # Check for stop loss / take profit hits
        if self.position:
            self._check_exits(timestamp, high, low, close)

        # Check for new signals
        signal = row.get("signal", SignalType.HOLD.value)

        if signal == SignalType.LONG.value and not self.position:
            self._open_position(
                PositionSide.LONG,
                timestamp,
                row["signal_price"],
                row.get("stop_loss"),
                row.get("take_profit"),
            )
        elif signal == SignalType.SHORT.value and not self.position:
            self._open_position(
                PositionSide.SHORT,
                timestamp,
                row["signal_price"],
                row.get("stop_loss"),
                row.get("take_profit"),
            )
        elif signal == SignalType.CLOSE_LONG.value and self.position:
            if self.position.side == PositionSide.LONG:
                self._close_position(timestamp, close, "signal_exit")
        elif signal == SignalType.CLOSE_SHORT.value and self.position:
            if self.position.side == PositionSide.SHORT:
                self._close_position(timestamp, close, "signal_exit")

        # Update equity
        self._update_equity(timestamp, close)

    def _open_position(
        self,
        side: PositionSide,
        timestamp: datetime,
        price: float,
        stop_loss: Optional[float],
        take_profit: Optional[float],
    ) -> None:
        """Open a new position."""
        # Apply slippage
        entry_price = price * (1 + self.slippage_rate) if side == PositionSide.LONG else price * (1 - self.slippage_rate)

        # Calculate position size - Use fixed percentage of initial capital
        position_value = self.initial_capital * 0.10  # 10% of capital per trade (realistic)
        quantity = position_value / entry_price

        # Calculate entry fees
        entry_fees = quantity * entry_price * self.commission_rate

        self.position = Position(
            side=side,
            entry_price=entry_price,
            quantity=quantity,
            entry_time=timestamp,
            stop_loss=stop_loss,
            take_profit=take_profit,
        )

        # Deduct position value from cash (fees will be deducted on close from P&L)
        self.cash -= position_value

        logger.debug(
            f"Opened {side.value} position: qty={quantity:.4f}, "
            f"price=${entry_price:.2f}, value=${position_value:.2f}"
        )

    def _close_position(
        self,
        timestamp: datetime,
        price: float,
        reason: str,
    ) -> None:
        """Close the current position."""
        if not self.position:
            return

        # Apply slippage
        exit_price = price * (1 - self.slippage_rate) if self.position.side == PositionSide.LONG else price * (1 + self.slippage_rate)

        # Calculate raw P&L (before fees)
        if self.position.side == PositionSide.LONG:
            raw_pnl = (exit_price - self.position.entry_price) * self.position.quantity
        else:  # SHORT
            raw_pnl = (self.position.entry_price - exit_price) * self.position.quantity

        # Calculate total fees (entry + exit)
        entry_fees = self.position.quantity * self.position.entry_price * self.commission_rate
        exit_fees = self.position.quantity * exit_price * self.commission_rate
        total_fees = entry_fees + exit_fees

        # Net P&L after fees
        net_pnl = raw_pnl - total_fees

        # Return to cash: original position value + net P&L
        position_initial_value = self.position.entry_price * self.position.quantity
        self.cash += position_initial_value + net_pnl

        # Calculate P&L percentage
        pnl_percent = (raw_pnl / position_initial_value) * 100

        # Record trade
        trade = Trade(
            side=self.position.side,
            entry_price=self.position.entry_price,
            exit_price=exit_price,
            quantity=self.position.quantity,
            entry_time=self.position.entry_time,
            exit_time=timestamp,
            pnl=net_pnl,  # Net P&L after all fees
            pnl_percent=pnl_percent,
            fees=total_fees,
            stop_loss=self.position.stop_loss,
            take_profit=self.position.take_profit,
            exit_reason=reason,
        )

        self.trades.append(trade)
        self.position = None

        logger.debug(
            f"Closed position: pnl=${net_pnl:.2f} ({pnl_percent:.2f}%), "
            f"reason={reason}"
        )

    def _check_exits(
        self,
        timestamp: datetime,
        high: float,
        low: float,
        close: float,
    ) -> None:
        """Check if stop loss or take profit was hit."""
        if not self.position:
            return

        if self.position.side == PositionSide.LONG:
            # Check stop loss
            if self.position.stop_loss and low <= self.position.stop_loss:
                self._close_position(timestamp, self.position.stop_loss, "stop_loss")
                return

            # Check take profit
            if self.position.take_profit and high >= self.position.take_profit:
                self._close_position(timestamp, self.position.take_profit, "take_profit")
                return

        else:  # SHORT
            # Check stop loss
            if self.position.stop_loss and high >= self.position.stop_loss:
                self._close_position(timestamp, self.position.stop_loss, "stop_loss")
                return

            # Check take profit
            if self.position.take_profit and low <= self.position.take_profit:
                self._close_position(timestamp, self.position.take_profit, "take_profit")
                return

    def _update_equity(self, timestamp: datetime, price: float) -> None:
        """Update equity curve."""
        equity = self.cash

        if self.position:
            if self.position.side == PositionSide.LONG:
                unrealized = (price - self.position.entry_price) * self.position.quantity
            else:  # SHORT
                unrealized = (self.position.entry_price - price) * self.position.quantity

            equity += unrealized

        self.equity = equity
        self.equity_curve.append((timestamp, equity))

    def _calculate_metrics(self) -> Dict[str, Any]:
        """Calculate performance metrics."""
        if not self.trades:
            return {
                "total_trades": 0,
                "win_rate": 0.0,
                "avg_win": 0.0,
                "avg_loss": 0.0,
                "profit_factor": 0.0,
                "sharpe_ratio": 0.0,
                "max_drawdown": 0.0,
                "max_drawdown_pct": 0.0,
            }

        # Basic stats
        total_trades = len(self.trades)
        winning_trades = [t for t in self.trades if t.pnl > 0]
        losing_trades = [t for t in self.trades if t.pnl < 0]

        win_rate = len(winning_trades) / total_trades if total_trades > 0 else 0
        avg_win = np.mean([t.pnl for t in winning_trades]) if winning_trades else 0
        avg_loss = np.mean([t.pnl for t in losing_trades]) if losing_trades else 0

        # Profit factor
        total_wins = sum(t.pnl for t in winning_trades)
        total_losses = abs(sum(t.pnl for t in losing_trades))
        profit_factor = total_wins / total_losses if total_losses > 0 else 0

        # Sharpe ratio (simplified - assumes daily returns)
        if len(self.equity_curve) > 1:
            equity_values = np.array([eq for _, eq in self.equity_curve])
            returns = np.diff(equity_values) / equity_values[:-1]
            sharpe = np.sqrt(365) * np.mean(returns) / (np.std(returns) + 1e-9)
        else:
            sharpe = 0.0

        # Max drawdown
        equity_values = np.array([eq for _, eq in self.equity_curve])
        running_max = np.maximum.accumulate(equity_values)
        drawdown = equity_values - running_max
        max_dd = np.min(drawdown)
        max_dd_pct = (max_dd / running_max[np.argmin(drawdown)] * 100) if len(running_max) > 0 else 0

        return {
            "total_trades": total_trades,
            "winning_trades": len(winning_trades),
            "losing_trades": len(losing_trades),
            "win_rate": win_rate * 100,
            "avg_win": avg_win,
            "avg_loss": avg_loss,
            "profit_factor": profit_factor,
            "sharpe_ratio": sharpe,
            "max_drawdown": max_dd,
            "max_drawdown_pct": max_dd_pct,
        }


__all__ = ["BacktestEngine", "Position", "Trade", "PositionSide"]
